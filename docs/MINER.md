# LogicNet: Miner documentation

### Overview

The Miner is responsible for solving the challenges generated by the Validator. The Miner will receive the challenges from the Validator, solve them, and submit the solutions back to the Validator. The Miner will be rewarded based on the number of challenges solved and the quality of the solutions.

**Miner is free to customize the solution**. We provide a default simple solution for the miner to start with. Miner can use the vLLM server to generate the solution.

**Protocol**: `LogicSynapse`. 
- Miner will be provided:
    - `logic_question`: The challenge generated by the Validator.
- Miner have to fill following content in the synapse to submit the solution:
    - `logic_reasoning`: The step by step reasoning to solve the challenge.
    - `logic_answer`: The final answer of the challenge as a short sentence.

**Reward Structure**:
- `correctness (bool)`: Validator ask LLM to check the matching between `logic_answer` and the ground truth.
- `similarity (float)`: Validator compute cosine similarity between `logic_reasoning` and validator's reasoning.
- `time_penalty (float)`: Penalty for late submission. It's ratio of `process_time / timeout * MAX_PENALTY`.

### Minimum Compute Requirements
- 1x GPU 24GB VRAM (RTX 4090, A100, A6000, etc)
- Storage: 100GB
- Python 3.10

Here's the revised chart sorted by ascending GPU footprint, including the model Qwen/Qwen2-7B-Instruct. Additionally, I've included a section on how to run larger models with lower VRAM using techniques such as adjusting `--gpu_memory_utilization`.

### Model to Run
Here are some model examples that could be leveraged, sorted by GPU footprint:

| Model Name | Model ID | Default GPU Footprint | Specialization |
| --- | --- | --- | --- |
| Qwen2-7B-Instruct | Qwen/Qwen2-7B-Instruct | 24 GB | Instruction-following, suitable for logic and structured reasoning |
| Mistral-7B-Instruct | mistralai/Mistral-7B-Instruct-v0.1 | 24 GB | High-performance, excellent for logical tasks |
| Qwen-7B-Chat | Qwen/Qwen-7B-Chat | 24 GB | Conversational logic and problem-solving |
| Baichuan2-13B-Chat | baichuan-inc/Baichuan2-13B-Chat | 32 GB | Versatile in language understanding, suitable for logic and math |
| Llama-2-13b-chat | meta-llama/Llama-2-13b-hf | 32 GB | Strong in conversational tasks, good for logic and structured reasoning |
| Falcon-40B | tiiuae/falcon-40b | 75 GB* | Advanced model, handles complex reasoning and logic efficiently |
| Mixtral-8x7B | mistralai/Mixtral-8x7B-Instruct-v0.1 | 92 GB* | Advanced model, handles complex reasoning and logic efficiently |

> \* Big models such as mixtral are very costly to run and optimize, so always bear in mind the trade-offs between model speed, model quality and infra cost.

### Setup for Miner
1. Git clone the repository
```bash
git clone https://github.com/LogicNet-Subnet/LogicNet logicnet
cd logicnet
```
2. Install the requirements
```bash
python -m venv main
. main/bin/activate
pip install -e .
pip uninstall uvloop -y
pip install git+https://github.com/lukew3/mathgenerator.git
```
3. Create env for vLLM
```bash
python -m venv vllm
. vllm/bin/activate
pip install vllm
```
3. Setup LLM Configuration

- For ease of use, you can run the scripts as well with PM2. To install PM2:
```bash
sudo apt update && sudo apt install jq && sudo apt install npm && sudo npm install pm2 -g && pm2 update
```
- Self host a vLLM server
```bash
. vllm/bin/activate
pm2 start "vllm serve Qwen/Qwen2-7B-Instruct --port 8000 --host 0.0.0.0" --name "sn35-vllm" # change port and host to your preference
```

- If you want to run larger models on GPUs with less VRAM, there are several techniques you can use to optimize GPU memory utilization:
    - You can adjust the GPU memory utilization to maximize the available memory by using a flag like `--gpu_memory_utilization`. This allows the model to use a specified percentage of GPU memory.
   ```bash
   pm2 start "vllm serve Qwen/Qwen2-7B-Instruct --gpu_memory_utilization 0.95 --port 8000 --host 0.0.0.0" --name "sn35-vllm" 
   # This command sets the model to use 95% of the available GPU memory.
   ```
   - Using mixed precision (FP16) instead of full precision (FP32) reduces the amount of memory required to store model weights, which can significantly lower VRAM usage.
   ```bash
   pm2 start "vllm serve Qwen/Qwen2-7B-Instruct --precision fp16 --gpu_memory_utilization 0.95 --port 8000 --host 0.0.0.0" --name "sn35-vllm"
   ```
   - If you have multiple GPUs, you can shard the model across them to distribute the memory load.
   ```bash
   pm2 start "vllm serve Qwen/Qwen2-7B-Instruct --shard --port 8000 --host 0.0.0.0" --name "sn35-vllm"
   ```

4. Run the following command to start mining
```bash
. main/bin/activate
pm2 start python --name "sn35-miner" -- neurons/miner/miner.py \
--netuid 35 --wallet.name "wallet-name" --wallet.hotkey "wallet-hotkey" \
--subtensor.network finney \
--axon.port "your-open-port" \
--miner.category Logic \ # specify the category to join. Currently, only Logic is supported
--miner.epoch_volume 50 \ # commit no of requests to be solved in an epoch. It will affect the reward calculation
--miner.llm_client.base_url http://localhost:8000/v1 \ # vLLM server base url
--miner.llm_client.model Qwen/Qwen2-7B-Instruct \ # vLLM model name
--logging.debug \ # Optional: Enable debug logging
```
